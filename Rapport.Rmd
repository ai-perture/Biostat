---
title: "Rapport"
author: "Ai-Ling Nguyen Bonnet & Elena Roques"
date: "2023-12-09"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(visdat)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(GGally)
library(factoextra)
library(pheatmap)
library(dplyr)
#library(outliers)
library(purrr)
library(mice)
library(tidyr)
library(caret)
library(questionr)
```

# Biostatistique : Etude du diabète

### Ai-Ling Nguyen Bonnet & Eléna Roques

Nous allons au cours de ce travail construire un modèle de prédiction du diabète à partir d'un jeu de données du NIDDK.

```{r}
data <- read.csv('diabetes.csv')
print(data, row.names=F)
```

Le jeu de données contient le diagnostic (diabétique ou non) ainsi que la mesure de 8 autres variables (âge, IMC, nombre de grossesses, pression...) sur un ensemble de 768 femmes.

# Question 0 - Prétraitement des données

On a remarqué ci-dessus que certaines données étaient manquantes (matérialisées par NA dans le data-frame).

On commence par regarder la quantité et la répartition de ces données manquantes :

```{r Données manquantes}
summary(data) #donne le nombre de NA par variable
vis_miss(data[,-1]) #graphe de répartition des NA par variable ; % de NA global et par variable
md.pattern(data, rotate.names=TRUE) #visualisation des NA par entrée ; entrées qui cumulent plusieurs NA
```

On remarque qu'effectivement, les variables insulin et triceps présentent un grand nombre de données manquantes, respectivement presque la moitié (374/768) et un tiers (227/768) des entrées.
Les autres variables sont complètes où avec une faible proportion de NA (5% ou moins) En tout, 9.4% des données sont manquantes.

Certaines entrées cumulent une absence de données dans 2, 3 ou 4 variables différentes.
Pour des raisons de cohérence des données, on décide d'écarter de la suite de l'étude celles où 3 ou 4 variables ne sont pas renseignées (soit 35 entrées).

```{r}
data$nb_NA <- apply(data, MARGIN = 1, function(x){sum(is.na(x))})
data_trie <- data |> subset(nb_NA < 3, select = - nb_NA)
```

On vérifie la répartition des NA restants :

```{r}
visu_NA <- vis_miss(data_trie)
grille <- md.pattern(data_trie, rotate.names=TRUE)
```

Il reste donc 7,4% de NA : la quasi-totalité concernent les variables insulin et triceps.

On va maintenant procéder à l'imputation des données pour données des valeurs à ces données manquantes.
Pour cela on utilise le package MICE : Multivariate Imputation by Chained Equations.

```{r}
data_impute=mice(data_trie, m=5) #construction des imputations = valeurs manquantes ; on le fait 5 fois
data_impute$meth #voir quelle méthode d'imputation a été utilisée pour chaque variable ; on peut spécifier si on veut utiliser une méthode en particulier
```

La méthode utlisée par défaut pour l'imputation des variables est le Predictive Mean Matching (pmm).
C'est une méthode qui permet d'obtenir des données manquantes probables en les tirant aléatoirement du reste des données parmi celles qui sont le plus proche de la valeur prédites en fonction des autres variables.

On a réalisé 5 imputations différentes, on va maintenant les observer pour vérifier qu'elles sont bien cohérentes.

```{r, echo = TRUE, fig.align = 'center', fig.height = 7.5, fig.width = 7.5}
dens_plot <- densityplot(data_impute, auto.key=TRUE)
strip_plot <- stripplot(data_impute, pressure + mass + glucose + triceps + insulin ~ .imp,cex = c(1.5), pch = c(20), jitter = TRUE, layout = c(5, 1))

grid.arrange(dens_plot, strip_plot, ncol = 1, nrow=2)
```

Les différentes imputations correspondent raisonnablement aux données.
On complète le jeu de données avec une des imputations et on vérifie que toutes les données manquantes ont été complétées.

```{r}
data = complete(data_impute, 1)
md.pattern(data, rotate.names=TRUE)
```

# Question 1 - Analyse exploratoire des données

## 1.1 - Distribution des variables

On va s'intéresser dans cette partie à la distribution des variables explicatives en fonction de la variable diabetes. On commence par calculer le nombre d'observation pour les deux groupes _pos_ et _neg_. Il y a quasiment deux fois plus d'observations négatives que positives, ce qui pourrait être pris en compte dans le modèle prédictif.
On continue en regardant la moyenne et la variance de chaque variable en fonction du diagnostic de la patiente.

```{r}
data%>%
  group_by(diabetes)%>%
  summarise(count = n())
```


```{r Moyennes}
data %>%
  group_by(diabetes) %>%
  summarise_at( c("pregnant", "glucose", "pressure", "triceps", "insulin", "mass","pedigree","age"),.f = list(mean = mean), na.rm = TRUE)

```

```{r Variances}
data %>%
  group_by(diabetes) %>%
  summarise_at( c("pregnant", "glucose", "pressure", "triceps", "insulin", "mass","pedigree","age"),.f = list(sd = sd), na.rm = TRUE)
```

### Représentation des distributions

On regarde aussi la distribution générale des variables selon le label *diabète*.
On peut déjà observer grossièrement des différences entre les deux groupes: la plus grosse étant sur la variable glucose où le groupe *pos* a globalement un taux plus élevé.
On note sur les autres variables (hors ID) que les *pos* ont des valeurs globalement plus élevées (nombre de grossesses, pression, triceps...).

```{r, echo = TRUE, fig.align = 'center', fig.height = 7.5, fig.width = 7.5}
data2 <- melt(data, id = "diabetes") 
ggplot(data = data2, aes(x = diabetes, y = value, color = diabetes)) + 
  geom_boxplot(col = "black", show.legend = FALSE, outlier.colour = NA) + theme_bw() +
  geom_point(shape = 1, 
             position = position_jitterdodge(dodge.width = .6, 
                                             jitter.width = .8), 
             size = 1.8, alpha = 1, show.legend = FALSE) +
  facet_wrap(. ~ variable, scales = "free") +
  theme(strip.background = element_rect(colour = "black", fill = "white"),
        strip.text.x = element_text(size = 11),
        axis.text = element_text(size = 9), axis.title = element_text(size = 0),
        legend.position = "bottom") + xlab("") + ylab("") +
  scale_color_manual(values = c("springgreen4","firebrick3"))
```

### Conditions du test statistique

On a deux groupes selon le diagnostic du diabète: les négatifs, *neg*, et les positifs,*pos*, dont on voudrait regarder les différences de moyenne.
de moyenne ?
on regarde pas que ça non ?
Pour cela, un test t de Student ou de Welch peut-être pertinent, pourvu que les conditions suivantes soient remplies:

1.  Indépendance des observations. Chaque sujet ne doit appartenir qu'à un seul groupe. Il n'y a aucun lien entre les observations de chaque groupe. C'est bien vérifié dans notre cas, puisque les observations ne peuvent avoir qu'un seul label *diabetes* (*pos* ou *neg*). Les observations sont faites sur des femmes différentes, supposées sans lien.
2.  Aucune valeur aberrante significative dans les deux groupes.
3.  Normalité. Les données pour chaque groupe devraient être distribuées approximativement normalement.

Le test de Student suppose que les variances entre les deux groupes sont similaires, ce qui n'est pas le cas du test de Welch.

### Valeurs aberrantes

On va commencer par observer les valeurs aberrantes pour chaque variable.
On ne considère pas les valeurs aberrantes sur l'age, la masse ou le nombre de grossesses car il peut y avoir des variables extrêmes en fonction du groupe d'étude qui ne seront pas des erreurs de mesure.

```{r Valeurs aberrantes}
# Variables à tester
variables_a_tester <- c("glucose", "pressure", "triceps", "insulin", "pedigree")

# Utiliser la fonction map() pour identifier les outliers pour chaque variable
outliers_list <- map(variables_a_tester, ~ data %>%
                        group_by(diabetes) %>%
                        identify_outliers(.x)%>%
                        mutate(variable = .x))

outliers_combined <- bind_rows(outliers_list, .id="Variables")%>%
  arrange(ID) %>%
  select(ID, variable,everything(), -c(Variables))%>%
  filter(is.extreme == TRUE) 
print(outliers_combined)

# Calculer le nombre de valeurs différentes dans la colonne "ID"
num_outliers <- outliers_combined %>%
  summarize(num_outliers= n_distinct(ID))

print(num_outliers)
```

Il y a moins de 20 observations aberrantes extrêmes.
On construit un set de données "nettoyées" sans ces valeurs :

```{r}
data <- anti_join(data, outliers_combined, by = "ID")
```

### Normalité par groupes

On vérifie, pour chaque variable, si les groupes de données suivent une loi normale.
On utilise à cet effet le test de Shapiro - Wilk.
Si la p-value est inférieure à 0.005, on considère que les données ne suivent pas une loi normale.

```{r Shapiro-Wilk test}
# Utiliser la fonction map() pour effectuer le test de Shapiro-Wilk sur chaque variable
shapiro_tests <- map(variables_a_tester, ~ data %>%
                      group_by(diabetes) %>%
                      summarise(p_value = shapiro.test(.[[.x]])$p.value) %>%
                      mutate(variable = .x)%>%
                      select(variable, diabetes, p_value))

# Combinez les résultats dans un tableau
shapiro_combine <- bind_rows(shapiro_tests)

# Afficher le tableau combiné
print(shapiro_combine)
```

La *p-value* est très faible pour tous les groupes donc on ne peut pas considérer que les données suivent une loi normale.

### Test statistique

Pour évaluer les différences entre les deux groupes *pos* et *neg* selon les variables, on utilise un test statistique.

Les données ne suivent pas une loi normale, donc le test de Welch ne peut pas être utilisé de façon rigoureuse.
On fait donc un test de Wilcoxon qui est une alternative non paramétrique à ces test t pour comparer des moyennes.
On s'autorise à faire le test de Welch pour comparer les résultats.

```{r Tests statistiques}
# Variables à tester
variables_a_tester <- c("pregnant", "glucose", "pressure", "triceps", "insulin", "mass", "pedigree", "age")

# Le t-test classique - version par défaut de Welch
welch_tests <- map(variables_a_tester, ~ data %>%
                    t_test(as.formula(paste(.x, "~ diabetes"))) %>%
                    add_significance())
welch_res <- bind_rows(welch_tests)%>% 
    select(-one_of("group1", "group2", "statistic","df"))%>%
    rename('Welch - p_value' = 'p', "Welch Signif" = 'p.signif')

# Le test de Wilcox pour des données non normalisées 
wilcox_tests <- map(variables_a_tester, ~ data %>%
                    wilcox_test(as.formula(paste(.x, "~ diabetes"))) %>%
                    add_significance())
wilcox_res <- bind_rows(wilcox_tests)%>% 
    select(-one_of("group1", "group2", "statistic"))%>%
  rename('Wilcox - p_value' = 'p', "Wilcox Signif" = 'p.signif')

test_res <- left_join(welch_res, wilcox_res)
print(test_res)
```

Pour chaque variables, et quelque soit le test, la *p-value* est très faible (inférieure à 0.001) donc on peut rejeter l'hypothèse que les groupes *positifs* et *négatifs* ont la même moyenne.

Il est donc raisonnable de considérer que pour chaque variable, la différence entre les deux groupes est significative.

## 1.2 - Analyse en composante principale

On va maintenant séparer le jeu de données avec les variables explicatives d'un côté et l'étiquette (diagnostic du diabète) de l'autre.
On enlève la colonne ID qui n'a pas d'interêt explicatif.

```{r Séparation variables-labels}
#On remplace les étiquettes de la colonne diabète par 0=neg ou 1=pos
data$diabetes <- factor(data$diabetes, levels = c("neg", "pos"), labels = c(0, 1))
#On sépare les données entre variables et sortie à expliquer
X = data[,2:(NCOL(data)-1)]
y = data[, NCOL(data)]
```

Les données sont normalisées et centrées au sein de la fonction pca.

```{r, echo = TRUE, fig.align = 'center', fig.height = 9, fig.width = 10}}
fit.pca <- prcomp(X, center = TRUE, scale = TRUE) #on construit le PCA en centrant (center) et normalisant (scale) les données

summary(fit.pca)
print(head(fit.pca$x))

fviz_screeplot(fit.pca, addlabels=TRUE)

indiv_pca <-fviz_pca_ind(fit.pca, habillage = data$diabetes) 
indiv_pca

fviz_pca_var(fit.pca, repel = TRUE)

fviz_pca_biplot(fit.pca, habillage = data$diabetes)

plotScree <-fviz_screeplot(fit.pca, addlabels=TRUE)
plotPcaIndiv <-fviz_pca_ind(fit.pca, habillage = data$diabetes) 
plotPcaVar <- fviz_pca_var(fit.pca, repel = TRUE)
plotPcaBiplot <- fviz_pca_biplot(fit.pca, habillage = data$diabetes)

grid.arrange(plotScree, plotPcaIndiv, plotPcaVar,plotPcaBiplot, ncol = 2, nrow=2)
```

### Interprétation

L'analyse en composante principale permet de représenter les données dans un plan en deux dimension qui est fonction des huit variables étudiées.
Les deux premières composantes principales permettent d'expliquer 49,85% de la variance des données.

Les variables les mieux représentées dans ces deux dimensions sont *age* et *pregnant* d'une part ; *triceps* et *mass* d'autre part (car longueur de la flèche proche de 1).
Ces deux couples de variables semblent corrélés car les flèches correspondantes pointent dans la même direction.

On va regarder cela de plus près en affichant la matrice des corrélations :

```{r}
corrplot(cor(X), type = "upper", addCoef.col = TRUE) #matrice des corrélations

ggpairs(data, columns = 2:(ncol(data)-1), ggplot2::aes(colour=diabetes), progress = FALSE)
```

On observe effectivement des corrélations \>0,55 pour trois couples de variables dont les deux cités ci-dessus ainsi que *glucose* et *insulin*, mais ces corrélations ne sont pas très significatives.
Dans le cas de *age* et *pregnant* on peut l'interpréter de la façon suivante : une femme qui a été enceinte de nombreuses fois est probablement âgée, mais une femme qui n'a jamais été enceinte n'est pas forcément jeune, d'où le fait que la corrélation ne soit pas très forte.

# Question 2 - Prédiction par régression logistique

## 2.1 Régression logistique

On commence par faire une base test indépendante, ie séparer les données en deux sets : training_data et test_data.
On va construire le modèle grâce au premier set puis le tester sur le deuxième.
Ici on prend un rapport 80/20% pour construire ces deux sets à partir des données exploitables.

Pour réaliser une régression logistique, il est nécessaire d'avoir un nombre suffisant de données par rapport au nombre de paramètres.
Ici, on a des plusieurs centaines d'observations (pour chaque label), pour huit paramètres donc on considère qu'il y a assez de données.

```{r Séparation en groupe test et groupe d'entraînement}
# On fixe une graine aléatoire pour la reproductibilité
set.seed(123)
# On génère un vecteur d'indices pour l'ensemble d'entraînement (80% des données)
indice <- sample(seq_len(nrow(data)), size = round(0.8 * nrow(data)), replace = FALSE)
# On crée l'ensemble d'entraînement
training_data <- data[indice, -1 ]
# On crée l'ensemble de test en excluant les indices de l'ensemble d'entraînement
test_data <- data[-indice, -1 ]
```

### Entrainement

```{r Régression logistique}
# On constuit le modèle de y en fonction de toutes les variables explicatives
glm_model <- glm(diabetes ~ pregnant +glucose + pressure + triceps + insulin + mass + pedigree + age, data = training_data, family = "binomial")
model_res <- summary(glm_model)
print(model_res$coefficients)
# On obtient l'ordre des indices des coefficients en ordre décroissant
order_indices <- order(abs(model_res$coefficients[, 1]), decreasing = TRUE)

# On affiche le résumé avec les coefficients triés
model_res$coefficients <- model_res$coefficients[order_indices, ]
print(model_res)
```

Comme la distribution de la variable Y est binomiale, le modèle de régression logistique se met sous équation sous la forme suivante : $$ 
logit(\mu) = log({\mu}/{1-\mu})= X\beta 
$$ avec $\mu$ la probabilité que Y=1 (i.e. que la patiente ait le diabète), X la matrice des données des variables explicatives et $\beta$ le vecteur des coefficients de la régression.

Ce qui donne : $$\mu = \frac{exp(\beta_0 +\beta_1 x_1 + ... + \beta_k x_k)}{1 + exp(\beta_0 + \beta_1 x_1 + ... + \beta_k x_k)}
$$

### Interprétation des coefficients

```{r, echo = TRUE, fig.align = 'center', fig.height = 5, fig.width = 7.5}
coefficients <- coef(glm_model)
or <- odds.ratio(glm_model)

# Créer un dataframe pour les coefficients
coefficients_df <- data.frame(variable = names(coefficients), coefficient = coefficients,odds_ratio = or$OR)

print(coefficients_df)

plotCoef <- ggplot(coefficients_df, aes(x = variable, y = coefficient)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  labs(title = "Graphique des Coefficients", x = "Variables", y = "Coefficient")
plotOR <- ggcoef_model(glm_model, exponentiate = TRUE)

grid.arrange(plotCoef, plotOR, ncol = 2)
```

On pourrait regarder les *odd ratios* OR qui représentent l'influence d'un paramètre --\> A creuser cf notes et références.

Le plus gros facteurs de risque est le pédigré en premier (coefficient et odd ration importants).
On trouve le facteur pregnant en deuxième.

### Prédictions


```{r}
probabilites_glm <- glm_model %>% 
  predict(test_data, type = "response")

<<<<<<< HEAD
predicted_classes <- ifelse(probabilites > 0.5, "pos", "neg")
predicted_classes <- factor(predicted_classes, levels = c("neg", "pos"), labels = c(0, 1))
=======

predicted_classes_glm <- ifelse(probabilites_glm > 0.5, "pos", "neg")
predicted_classes_glm <- factor(predicted_classes_glm, levels = c("neg", "pos"), labels = c(0, 1))
>>>>>>> 0304ce30a8c63965eff2cdd37034c2c4c4c6ec9c
```



```{r}
confusion_glm = confusionMatrix(data=predicted_classes_glm, reference = test_data$diabetes)
confusion_glm
```

<<<<<<< HEAD
A ce stade, l'accuracy est plutôt pas mal (>75%).
=======
A ce stade, l'accuracy est d'environ 71%.
>>>>>>> 0304ce30a8c63965eff2cdd37034c2c4c4c6ec9c

## 2.2. Calculs

cf YVANN

Intéressons nous à une pénalité de type l_1 +l_2.

Soit $\beta_0$ le minimiseur de $f: \beta \rightarrow || y - x\beta||_2^2$ .
On cherche le minimiseur de $\phi : \beta \rightarrow || y - x\beta||_2^2 + \lambda_1||\beta||_1 + \lambda_2||\beta||_2^2$.
On suppose que $x^Tx = I$.

### Expression de $\beta_0$

$f$ est lisse donc si $\beta_0$ minimise $f$, alors: $d_{\beta_0}f.v = 0$.
Or $$
d_{\beta_0}f.v = 2<-xv|y-x\beta_0> = 2<v|x^T(x\beta_0-y)> = 2<v|\beta_0 - x^Ty>
$$ D'où $\beta_0 - x^Ty=0$, soit :

$$
\beta_0 = x^Tv
$$

### Minimiseur avec pénalité

Pour tout $\beta$:

$$
||y-x\beta||_2^2 = ||y||_2^2 -2<y|x\beta> + ||x\beta||_2^2 
$$

Comme $x^Tx = 1>$ et $\beta_0 = x^Ty$ alors:

$$
||y-x\beta||_2^2 = ||y||_2^2 -2<\beta_0|\beta> + ||\beta||_2^2 
$$

Donc

$$
\phi : \beta \rightarrow ||y||_2^2 -2<\beta_0|\beta> + \lambda_1 ||\beta||_1 + (1+\lambda_2)||\beta||_2^2 
$$ Ainsi minimiser $\phi$ revient à minimiser

$$
\psi : \beta \rightarrow (1+\lambda_2)||\beta||_2^2 + \lambda_1||\beta||_1 - 2<\beta_0,\beta>
$$

Pour tout $\beta$ :

$$
\psi(\beta) = \sum_{k} (1+\lambda_2)\beta_k + \lambda_1|\beta_k| - 2\beta_k\beta_{0,k}
$$

Il est clair que $\beta$ minimise $\psi$ si et seulement si pour tout $k$, $\beta_k$ minimise $g(\beta_k, \beta_{0,k}) = (1+\lambda_2)\beta_k + \lambda_1|\beta_k| - 2\beta_k\beta_{0,k}$.

Pour $\beta_k >0$:

$$
g(\beta_k, \beta_{0,k}) = (1+\lambda_2)\beta_k^2 + (\lambda_1-2\beta_{0,k})\beta_k
$$ Donc $$\beta_k = \frac{\beta_{0,k}- \frac{\lambda_1}{2}}{1 + \lambda_2}$$ et $$\beta_{0,k}> \frac{\lambda_1}{2}$$

Pour $\beta_k >0$:

$$
g(\beta_k, \beta_{0,k}) = (1+\lambda_2)\beta_k^2 - (\lambda_1+2\beta_{0,k})\beta_k
$$ Donc $$\beta_k = \frac{\beta_{0,k}+ \frac{\lambda_1}{2}}{1 + \lambda_2}$$ et $$\beta_{0,k}< \frac{\lambda_1}{2}$$

Si $\beta_{0,k} = \frac{\lambda_1}{2}$ alors $\beta_k = 0$

On peut résumer ces différents cas sous la forme suivante:

$$
\beta_k = \frac{max (0,|\beta_{0,k}| - \frac{\lambda_1}{2})}{1+\lambda_2}.sign(\beta_{0,k})
$$

## 2.3. Pénalité ℓ1 + ℓ2.

<<<<<<< HEAD
Caractère parcimonieux = condition mathématique que le modèle doit vérifier.

Trouver un cours -\> La démo peut se trouver :-) (on va pas la faire nous meme quand meme)(hihi).
=======
La pénalité ainsi appliquée favorise les coefficients nuls dès que $\beta_{0,k}$ est plus petit en valeur absolue que $\frac{\lambda_1}{2}$ donc elle est parcimonieuse.
>>>>>>> 0304ce30a8c63965eff2cdd37034c2c4c4c6ec9c

## 2.4. Modèle de régression logistique pénalisée ℓ1 +ℓ2

Pour ce modèle plus sophistiqué, on fera une régression pénalisée avec la fonction glmnet. On optimise plusieurs paramètres:
- Le poids des classes, car elles sont de tailles inégales. Ils ont été optimisés à la main, et on a obtenu un factor 3 entre _neg_ et _pos_. 
- Le facteur lambda est optimisé par cross-validation, la valeur retenue étant 0.01.
- Plusieurs valeurs d'alpha ont été testées. On remarque une accuracy plus élevée pour alpha = 0, ie une régression ridge (L2) simple. Cela favorise un modèle où tous les coefficients ont tendance à être réduits de manière égale, sans favoriser la sélection de variables spécifiques.

Quelle que soit la valeur de lambda, les variables les plus influentes sont le glucose et la masse. 
```{r Données train et test}
# Créer les matrices de caractéristiques d'entraînement et de test ainsi que les vecteurs de réponses

X_train <- as.matrix(training_data[, -ncol(training_data)])
y_train <- as.numeric(training_data[, ncol(training_data)]) - 1
weights <- ifelse(y_train == 0, 1, 3)
X_test <- as.matrix(test_data[, -ncol(test_data)])
y_test <- test_data[, ncol(test_data)]
# Standardisation des données (optionnel, mais souvent recommandé)
X_train <- scale(X_train)
X_test <- scale(X_test)

```


```{r Validation croisée }

library(glmnet)

# Définir la grille de valeurs pour alpha et lambda
lambdas <- seq(0, 1, by = 0.001)  # Valeurs de lambda de 10 à 0.01 en échelle logarithmique

# Appliquer la validation croisée pour trouver les meilleurs hyperparamètres
cv_model <- cv.glmnet(X_train, y_train, alpha = 0.5, lambda = lambdas)#alpha = alphas,

# Afficher les meilleurs hyperparamètres
best_lambda <- cv_model$lambda.min
cat("Meilleur lambda:", best_lambda, "\n")
```

```{r l1 + l2}
# Entraîner le modèle avec les meilleurs hyperparamètres sur l'ensemble d'entraînement complet
glmnet_model <- glmnet(X_train, y_train,  lambda = best_lambda, alpha = 0.5, weights = weights)#

# Prédictions sur les données de test
predictions_glmnet <- predict(glmnet_model, newx = X_test, s = best_lambda)
predicted_classes_glmnet <- ifelse(predictions_glmnet > 0.5, "pos", "neg")
predicted_classes_glmnet <- factor(predicted_classes_glmnet, levels = c("neg", "pos"), labels = c(0, 1))

# Calcul de l'erreur quadratique moyenne (MSE) sur les données de test
mse <- mean((predictions_glmnet - (as.numeric(y_test)-1))^2)
cat("Mean Squared Error:", mse, "\n")

# Coefficients du modèle
coef(glmnet_model)
confusion_glmnet = confusionMatrix(data=predicted_classes_glmnet, reference = y_test)
confusion_glmnet
```

```{r l1 seul}
# Entraîner le modèle avec les meilleurs hyperparamètres sur l'ensemble d'entraînement complet
best_model <- glmnet(X_train, y_train,  lambda = best_lambda, alpha = 0, weights = weights)

# Prédictions sur les données de test
predictions3 <- predict(best_model, newx = X_test, s = best_lambda)
predicted_classes3 <- ifelse(predictions3 > 0.5, "pos", "neg")
predicted_classes3 <- factor(predicted_classes3, levels = c("neg", "pos"), labels = c(0, 1))

# Calcul de l'erreur quadratique moyenne (MSE) sur les données de test
mse <- mean((predictions3 - (as.numeric(y_test)-1))^2)
cat("Mean Squared Error:", mse, "\n")

# Coefficients du modèle
coef(best_model)
confusionMatrix(data=predicted_classes3, reference = y_test)

```



Pour le modèle alpha = 0.5: 
- le modèle pénalise seulement la variable _pressure_
- la balanced accuracy est meilleure Balanced Accuracy : 0.7328 au lieu de 0.6682
> créer une table pour comparer la régression simple et la régression pénalisée 



## 2.5. Comparaison avec la selection de variable pas - à - pas

Comparer vos résultats issus de la régression logistique pénalisée ℓ1 + ℓ2 au résultat d'une méthode de

sélection de variables pas-à-pas (step() en R).

```{r méthode step()}
# Créer un modèle linéaire initial
initial_model <- glm(diabetes ~ pregnant +glucose + pressure + triceps + insulin + mass + pedigree + age, data = training_data, family = "binomial")

step_model <- step(initial_model)

# Obtenir les noms des variables incluses dans le modèle final
included_variables <- names(coefficients(step_model))
print(included_variables)
coef(step_model)



```


```{r}
# step_model <- lm(diabetes ~ pregnant +glucose +triceps +  mass + pedigree + age, data = training_data, family = "binomial")
# coef(step_model)

probabilites_step <- step_model %>% 
  predict(test_data, type = "response")

predicted_classes_step <- ifelse(probabilites_step > 0.5, "pos", "neg")
predicted_classes_step <- factor(predicted_classes_step, levels = c("neg", "pos"),labels = c(0, 1))

confusion_step = confusionMatrix(data=predicted_classes_step, reference = test_data$diabetes)
confusion_step
```
## 2.6 Conclusion

### Comparaison des différents modèles 


```{r}
calculate_F1 <- function(confusion_matrix) {

  precision <- confusion_matrix$byClass['Precision']
  recall <- confusion_matrix$byClass['Recall']
  f1_score <- 2 * (precision * recall) / (precision + recall)


  return(f1_score)
}
```

```{r}
metrics_df <- data.frame(
  Model = c("GLM", "GLMNet", "Step"),
  Precision = c(confusion_glm$byClass['Precision'], confusion_glmnet$byClass['Precision'], confusion_step$byClass['Precision']),
  Recall = c(confusion_glm$byClass["Recall"], confusion_glmnet$byClass["Recall"], confusion_step$byClass["Recall"]),
  F1_Score = c(calculate_F1(confusion_glm),calculate_F1(confusion_glmnet),calculate_F1(confusion_step)),
  Balanced_Accuracy = c(confusion_glm$byClass["Balanced Accuracy"], confusion_glmnet$byClass["Balanced Accuracy"], confusion_step$byClass["Balanced Accuracy"]),
  stringsAsFactors = FALSE
)

metrics_df
```

Finalement, GLMNet a les moins bons scores en F1 et balanced accuracy. Sa précision est bonne mais le recall est trop mauvais par rapport aux autres. 
