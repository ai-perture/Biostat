---
title: "L'analyse en Composantes principales (ACP) et le clustering hiérarachique"
author: "VSE Students"
date: '`r Sys.Date()`'
output:
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analyse multidimensionnelle des données du myocarde  

Dans ce document, on réalise une analyse multdimensionnel des données du myocardes. Deux méthodes sont explorés : l'Analyse en Composantes Principales et le Clustering hiérarchique.

```{r, message = FALSE}

#Chargement des packages utiles
library(reshape2)
library(ggplot2)
library(corrplot)
library(GGally)
library(factoextra)
library(pheatmap)
```

## Analyse en Composantes Principales (ACP)

**Liste des principales commandes R pour l'ACP avec prcomp() :**

* prcomp(x): ACP de la matrice x (variables numériques) 
* prcomp(x, scale = TRUE): scale = TRUE pour l'ACP normée (FALSE par défaut) 
* plot(prcomp(x))
* summary(prcomp(x, scale = TRUE))
* biplot(prcomp(x, scale = TRUE))

**Description de la base :**

On dispose des données de 71 victimes d'infarctus du myorcarde (29 décès, 42 survivants) pour lesquelles ont été mesurées à leur admission dans un service de cardiologie les 7 variables suivantes :

* (1) Fréquence cardiaque - FRCAR
* (2) Index cardiaque - INCAR
* (3) Index systolique - INSYS
* (4) Pression diastolique - PRDIA
* (5) Pression artérielle pulmonaire - PAPUL
* (6) Pression ventriculaire - PVENT
* (7) Résistance pulmonaire - REPUL
* Facteur PRONO (DECES/SURVIE) 

Données de J.-P. Nakache reprises dans G. Saporta (Ed. Technip)
  
**Question  1.** Importez le fichier de données "data/myocarde.csv". Regardez les principales caractéristiques de distribution des variables du tableau. 

```{r, echo = TRUE}
myocarde <- read.csv("myocarde.csv", header = TRUE, sep = ";")
X = myocarde[, - NCOL(myocarde)]
y = myocarde[, NCOL(myocarde)]
str(myocarde)
summary(myocarde)
```

Pour commencer à analyser simplement le jeu de données on peut regarder les moyennes des variables selon le pronostic vital du patient. 

```{r}
apply(X, 2, function(x) tapply(x, list(y), mean))
```

Et fournir une représentation graphique des distributions en fonction du pronostic. 
  
```{r, echo = TRUE, fig.align = 'center', fig.height = 7.5, fig.width = 7.5}
myocarde2 <- melt(myocarde, id = "PRONO") 
ggplot(data = myocarde2, aes(x = PRONO, y = value, color = PRONO)) + 
  geom_boxplot(col = "black", show.legend = FALSE, outlier.colour = NA) + theme_bw() +
  geom_point(shape = 1, 
             position = position_jitterdodge(dodge.width = .6, 
                                             jitter.width = .8), 
             size = 1.8, alpha = 1, show.legend = FALSE) +
  facet_wrap(. ~ variable, scales = "free") +
  theme(strip.background = element_rect(colour = "black", fill = "white"),
        strip.text.x = element_text(size = 11),
        axis.text = element_text(size = 9), axis.title = element_text(size = 0),
        legend.position = "bottom") + xlab("") + ylab("") +
  scale_color_manual(values = c("firebrick3", "springgreen4"))
```

Pour aller plus loin, on peut également étudier les corrélations entre les variables du tableau. 

```{r, echo = TRUE, fig.align = 'center', fig.height = 4.5, fig.width = 4.5}
mcor <- cor(X)
mcor
```

Il est possible de visualiser cette matrice de corrélation en utilisant le package `corrplot`.

```{r}
corrplot(mcor, type = "upper")
```

On peut aussi visualiser les relations entre paires de variables par le graphique suivant :

```{r, echo = TRUE, fig.align = 'center', fig.height = 7.5, fig.width = 8.5}
mycol <- as.numeric(myocarde$PRONO == "DECES") + 1

ggpairs(data = myocarde, columns = 1:(ncol(myocarde)-1), ggplot2::aes(colour = PRONO), progress = FALSE)
```

Pour aller plus loin dans l'analyse multidimensionnelle des données, on va maintenant réaliser une ACP sur les données standardisées à l'aide de la fonction prcomp(). 

```{r, echo = TRUE, fig.align = 'center', fig.height = 5.5, fig.width = 5.5}
fit.pca <- prcomp(X, scale = TRUE)
```

Un premier résumé du jeu de données est donné ci-dessous
```{r}
summary(fit.pca)
```

Les axes principaux, c'est-à-dire, les vecteurs propres de la matrice des corrélations sont reportés ci-dessous

```{r}
fit.pca$rotation
```

Enfin, on accède aux composantes principales (i.e. aux coordonnées des individus projectés sur les axes principaux) comme suit

```{r}
head(fit.pca$x)
```

On visualise le pourcentage de variance capturé par les composantes principales

```{r}
fviz_screeplot(fit.pca, addlabels=TRUE)
```

On visualise également la carte des indididus sur le premier plan principal.

```{r}
fviz_pca_ind(fit.pca, habillage = myocarde[, "PRONO"])
```

Pour apprécier la contribution des variables à la contructions des composantes, on reporte le cercle des corrélations.

```{r}
fviz_pca_var(fit.pca, repel = TRUE)
```

Le biplot permet d'obtenir une figure synthétique permettant de "superposer" la carte des indivuds et le cercle des corrélations

```{r}
fviz_pca_biplot(fit.pca, habillage = myocarde[, "PRONO"])
```

## Classification Ascendante hiérarchique

On souhaite maintenant réaliser une classification ascendante hiérarchique du jeu de données `myocarde`.

**Liste des principales commandes R pour la Classification Hiérarchique  :**

* hclust(x): Clustering hiérarchique du jeu de données
* dist(x, method = "euclidean") pour calculé une matrice de distance entre individus
* heatmap(x) permet de visualiser par l'intermédiare d'une "carte de chaleur" les données.

On va appliquer la méthode de Ward sur les données  `myocarde`. 

Il convient dans un premier temps de calculer la matrice sur les centrées réduites.

```{r, echo = TRUE, fig.align = 'center', fig.height = 3, fig.width = 7}
# hierarchical clustering on Myocarde
X = scale(X)
dmat <- dist(X, method = "euclidean")^2
```

On visualiste cette matrice des distances grace au package `pheatmap`.

```{r}
#data visualisation
pheatmap(as.matrix(dmat), 
         cluster_rows = FALSE, 
         cluster_cols = FALSE, 
         fontsize=6
         )
```

On réalise ensuite un clustering hiérarchique en utilisant la méthode de Ward.

```{r}
fit.hc <- hclust(dmat, method = "ward.D")
```


On peut visualiser ensuite le dendogramme associé

```{r}
fviz_dend(fit.hc, cex = 0.5, k_colors = "black")
```


On distingue ici "clairement" trois groupes d'individus. La figure suivante 
permet d'apprécier l'inertie intra-groupe totale en fonction du nombre de cluster 

```{r}
fviz_nbclust(X, hcut, method = "wss")
```

et le critère du coude confirme le découpage en 3 clusters.


Le dendogramme ensuite coloré en fonction de ces trois groupes est représenté sur 
la figure ci-dessous :

```{r}
fviz_dend(fit.hc, k=3, rect = TRUE, cex = 0.5,
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"))
```


Enfin, on utilise la variable `PRONO` pour colorer les feuilles de l'arbre.

```{r, message = FALSE}
fviz_dend(fit.hc, k=3, rect = TRUE, cex = 0.5,
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"), 
          label_cols = as.numeric(factor(myocarde$PRONO))[fit.hc$order])
```

On peut maintenant tenter une interprétation de ces résultats.

```{r}
table(myocarde[, "PRONO"], cutree(fit.hc, 3))
```

On caractérise les groupes en regardant la moyenne de chaque variable conditionnellement aux groupes.

```{r}
apply(myocarde[, -NCOL(myocarde)], 2, 
      function(x) tapply(x, list(cutree(fit.hc, 3)), mean))
```

Pour conclure et améliorer encore d'avantage l'interprétation des résultats, on peut visualiser les groupes sur le premier plan principal.


```{r}
fviz_pca_biplot(fit.pca, habillage = cutree(fit.hc, 3))
```

**Remarque**: Il est tout à fait possible de réaliser une classification hiérarchique non sur les données d'origines mais sur les composantes principales.... à faire en exercice.
